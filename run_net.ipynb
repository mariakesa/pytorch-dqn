{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from gym.wrappers import Monitor\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from model import Model\n",
    "from environment import Environment\n",
    "from utils import Memory, EpsilonScheduler, make_log_dir, save_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(q_func,DEVICE,NUM_TEST,env,save=False):\n",
    "    print(\"[TESTING]\")\n",
    "    total_reward = 0\n",
    "    unclipped_reward = 0\n",
    "\n",
    "    for i in range(NUM_TEST):\n",
    "        if i == 0 and save:\n",
    "            frames = []\n",
    "\n",
    "        env.reset(eval=True) # performs random actions to start\n",
    "        state, _, done, _ = env.step(env.action_space.sample())\n",
    "        frame = 0\n",
    "\n",
    "        while not done:\n",
    "            if i == 0 and save:\n",
    "                frames.append(state[0,0])\n",
    "            \n",
    "            # env.render()\n",
    "            q_values = q_func(state.to(DEVICE))\n",
    "            if np.random.random() > 0.01: # small epsilon-greedy, sometimes 0.05\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            lives = env.ale.lives()\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if env.ale.lives() != lives: # lost life\n",
    "                pass\n",
    "                # plt.imshow(next_state[0,0])\n",
    "                # plt.savefig(f\"frame-{frame}.png\")\n",
    "                # print(\"LOST LIFE\")\n",
    "\n",
    "            unclipped_reward += info['unclipped_reward']\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            frame += 1\n",
    "            # print(f\"[TESTING {frame}] Action: {action}, Q-Values: {np.array(q_values.cpu().detach())}, Reward: {reward}, Total Reward: {total_reward}, Terminal: {done}\")\n",
    "            # plt.imshow(state[0,0])\n",
    "            # plt.savefig(\"frame-{}.png\".format(frame))\n",
    "\n",
    "        if i == 0 and save:\n",
    "            frames.append(state[0,0])\n",
    "            save_gif(frames, \"{}.gif\".format(os.path.join(video_dir, str(scheduler.step_count()))))\n",
    "\n",
    "    total_reward /= NUM_TEST\n",
    "    unclipped_reward /= NUM_TEST\n",
    "    print(f\"[TESTING] Total Reward: {total_reward}, Unclipped Reward: {unclipped_reward}\")\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net():\n",
    "    \n",
    "    weights_path='/home/maria/Documents/pytorch-dqn/weights/breakout/good.pt'\n",
    "    \n",
    "    MEM_SIZE = int(1e6) # this is either 250k or 1 million in the paper (size of replay memory)\n",
    "    EPISODES = int(1e5) # total training episodes\n",
    "    BATCH_SIZE = 32 # minibatch update size\n",
    "    GAMMA = 0.99 # discount factor\n",
    "    STORAGE_DEVICES = ['cpu'] # list of devices to use for episode storage (need about 10GB for 1 million memories)\n",
    "    DEVICE = 'cpu' # list of devices for computation\n",
    "    UPDATE_FREQ = 4 # perform minibatch update once every UPDATE_FREQ\n",
    "    TARGET_UPDATE_EVERY = 10000 # in units of minibatch updates\n",
    "    INIT_MEMORY_SIZE = 200000 # initial size of memory before minibatch updates begin\n",
    "\n",
    "    TEST_EVERY = 1000 # (episodes)\n",
    "    PLOT_EVERY = 10 # (episodes)\n",
    "    SAVE_EVERY = 1000 # (episodes)\n",
    "    EXPERIMENT_DIR = \"experiments\"\n",
    "    NUM_TEST = 1\n",
    "    GAME = 'breakout'\n",
    "    \n",
    "    env = Environment(game=GAME)\n",
    "    #mem = Memory(MEM_SIZE, storage_devices=STORAGE_DEVICES, target_device=DEVICE)\n",
    "\n",
    "    q_func = Model(env.action_space.n).to(DEVICE)\n",
    "    q_func.load_state_dict(torch.load(weights_path,map_location='cuda:0'))\n",
    "\n",
    "    target_q_func = Model(env.action_space.n).to(DEVICE)\n",
    "    target_q_func.load_state_dict(q_func.state_dict())\n",
    "    \n",
    "    test(q_func,DEVICE,NUM_TEST,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESTING]\n",
      "[TESTING] Total Reward: 96.0, Unclipped Reward: 384.0\n"
     ]
    }
   ],
   "source": [
    "run_net()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
